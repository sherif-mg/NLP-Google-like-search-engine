{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2goQ7zZ6Jky"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8kSUsoO1Z3U"
      },
      "source": [
        "## download the word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WbgU0u66nw4",
        "outputId": "157fbd48-a31b-448f-c9a0-a558bbeb6edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wikipedia-api\n"
      ],
      "metadata": {
        "id": "CBJWhVTHF_Bb",
        "outputId": "97c79d2a-18a8-4216-a964-6b13b9ffe4b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=6c218e0992737eba71f61273a3f06bd34a952412cd7125bc43aaaa10fc58b3e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get data "
      ],
      "metadata": {
        "id": "dcPGSdngAuo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "wiki_wiki = wikipediaapi.Wikipedia('en')"
      ],
      "metadata": {
        "id": "193oPxWkGavC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages=[wiki_wiki.page('Cristiano Ronaldo'),\n",
        "wiki_wiki.page('Lionel Messi'),\n",
        "wiki_wiki.page('Mohamed Salah'),\n",
        "wiki_wiki.page('Karim Benzema'),\n",
        "wiki_wiki.page('Neymar'),\n",
        "wiki_wiki.page('Samsung'),\n",
        "wiki_wiki.page('Apple Inc.'),\n",
        "wiki_wiki.page('OnePlus'),\n",
        "wiki_wiki.page('Nokia'),\n",
        "wiki_wiki.page('Xiaomi'),\n",
        "wiki_wiki.page('Facebook'),\n",
        "wiki_wiki.page('Instagram'),\n",
        "wiki_wiki.page('WhatsApp'),\n",
        "wiki_wiki.page('Snapchat'),\n",
        "wiki_wiki.page('TikTok'),\n",
        "wiki_wiki.page('The Weeknd'),\n",
        "wiki_wiki.page('Sam Smith'),\n",
        "wiki_wiki.page('Shawn Mendes'),\n",
        "wiki_wiki.page('Justin Bieber'),\n",
        "wiki_wiki.page('Zayn Malik'),\n",
        "wiki_wiki.page('Paris'),\n",
        "wiki_wiki.page('London'),\n",
        "wiki_wiki.page('Kabul'),\n",
        "wiki_wiki.page('Madrid'),\n",
        "wiki_wiki.page('Ankara')]"
      ],
      "metadata": {
        "id": "8iMDH59RKHGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={}\n",
        "for i in pages:\n",
        "  data[i.title]=i.text"
      ],
      "metadata": {
        "id": "4EZBPxTFDVTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "    stopwords=set(stopwords.words('english'))\n",
        "    for i in data:\n",
        "      data[i]=data[i].lower()\n",
        "      data[i]=re.sub('\\n', ' ', data[i])\n",
        "      data[i]=re.sub('[^a-zA-Z\\s]*', '', data[i])\n",
        "      data[i]=data[i].split()\n",
        "      data[i]=[word for word in data[i] if word not in stopwords]     \n",
        "      \n",
        "    return data  \n"
      ],
      "metadata": {
        "id": "-fiI4BY0DiPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Training(data):\n",
        "    import numpy as np\n",
        "    for i in data:\n",
        "      lst=[]\n",
        "      for j in data[i]:\n",
        "        if(j in word2vec_model.vocab):\n",
        "           lst.append(word2vec_model[j])\n",
        "      data[i]=np.array(lst)  \n",
        "    return data"
      ],
      "metadata": {
        "id": "_EewvyVGXTaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg(data):\n",
        "    for i in data:\n",
        "      data[i]=np.mean(data[i], axis = 0)\n",
        "    return data  "
      ],
      "metadata": {
        "id": "dL2GOGDRde2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(data):\n",
        "  data=preprocessing(data)\n",
        "  data=Training(data)\n",
        "  #make each doc as one vec\n",
        "  data=avg(data)\n",
        "  return data"
      ],
      "metadata": {
        "id": "OBjDXwlHeIr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=main(data)"
      ],
      "metadata": {
        "id": "XFO78nu9Fkgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1732899f-1d93-4c19-9a6a-75ef785ea1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
        "df.to_csv(\"data.csv\")\n"
      ],
      "metadata": {
        "id": "z-NZlDnLyoBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff = pd.read_csv(\"data.csv\", index_col=0)\n",
        "d = dff.to_dict(\"split\")\n",
        "d = dict(zip(d[\"index\"], d[\"data\"]))\n",
        "for i in d:\n",
        "  d[i]=np.array(d[i])"
      ],
      "metadata": {
        "id": "a-oH9seMy6eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mysentence={1:\"capital of spain\",2:\"the best football player in world\",3:\"a British singer\"}"
      ],
      "metadata": {
        "id": "bMOvtfER1S0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mysentence=main(mysentence)"
      ],
      "metadata": {
        "id": "TTbpNOui2Tin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8671c10d-3423-403b-f0bd-7fab31545c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "for i in mysentence:\n",
        "  result={}\n",
        "  for j in data:\n",
        "    #compute similarity by cosine similarity\n",
        "    result[j] = dot(mysentence[i], data[j])/(norm(mysentence[i])*norm(data[j]))\n",
        "  sort_orders = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
        "  print(sort_orders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3axQX5o2hup",
        "outputId": "b1f7b173-d0b6-4551-9ca7-418bcf7f4e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Madrid', 0.59354836), ('Paris', 0.5418557), ('London', 0.53775483), ('Kabul', 0.47338113), ('Samsung', 0.4477142), ('Ankara', 0.43869993), ('Cristiano Ronaldo', 0.43330616), ('Lionel Messi', 0.42871916), ('Neymar', 0.4246755), ('Karim Benzema', 0.42206714), ('Nokia', 0.41587657), ('Mohamed Salah', 0.4121612), ('Xiaomi', 0.39238426), ('OnePlus', 0.37895733), ('Facebook', 0.35907766), ('Apple Inc.', 0.35819224), ('WhatsApp', 0.3455623), ('Sam Smith', 0.34451327), ('Zayn Malik', 0.34421867), ('Justin Bieber', 0.34166652), ('TikTok', 0.34031108), ('Shawn Mendes', 0.33523044), ('The Weeknd', 0.2965294), ('Instagram', 0.29374656), ('Snapchat', 0.2537795)]\n",
            "[('Cristiano Ronaldo', 0.66101503), ('Lionel Messi', 0.6571004), ('Mohamed Salah', 0.6504365), ('Neymar', 0.6454509), ('Karim Benzema', 0.627401), ('Shawn Mendes', 0.46356764), ('Justin Bieber', 0.46152693), ('Zayn Malik', 0.4582121), ('The Weeknd', 0.4488836), ('Madrid', 0.44015992), ('Sam Smith', 0.43829978), ('London', 0.42733732), ('OnePlus', 0.41148058), ('Apple Inc.', 0.406351), ('Ankara', 0.40177748), ('TikTok', 0.40081522), ('Samsung', 0.39057723), ('Paris', 0.39027894), ('Xiaomi', 0.38895684), ('Snapchat', 0.38567325), ('Instagram', 0.37179706), ('Nokia', 0.36969164), ('Facebook', 0.36426586), ('Kabul', 0.35888997), ('WhatsApp', 0.35087958)]\n",
            "[('Sam Smith', 0.5294286), ('Zayn Malik', 0.51289684), ('Justin Bieber', 0.49166673), ('Shawn Mendes', 0.4639043), ('The Weeknd', 0.4499088), ('London', 0.41007623), ('Madrid', 0.3777297), ('Paris', 0.37732622), ('Kabul', 0.36024755), ('Ankara', 0.33707833), ('Samsung', 0.32633135), ('TikTok', 0.32415476), ('Xiaomi', 0.32159218), ('OnePlus', 0.31989068), ('Neymar', 0.31821722), ('Nokia', 0.3148976), ('Cristiano Ronaldo', 0.31052777), ('Facebook', 0.31035212), ('Mohamed Salah', 0.30683705), ('Lionel Messi', 0.2936277), ('Karim Benzema', 0.2908992), ('Apple Inc.', 0.2900986), ('WhatsApp', 0.2853419), ('Instagram', 0.26009184), ('Snapchat', 0.23768629)]\n"
          ]
        }
      ]
    }
  ]
}